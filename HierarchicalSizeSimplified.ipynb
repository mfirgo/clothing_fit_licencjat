{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_preparation import get_train_runttherunway_data, get_test_runttherunway_data, get_processed_renttherunway_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from os.path import exists\n",
    "from os import listdir, remove\n",
    "import re\n",
    "def question(question_text):\n",
    "    answer = input(question_text+ \"[y/n] \").lower()\n",
    "    if answer in [\"yes\", \"y\", \"ye\"]:\n",
    "        return True\n",
    "    if answer in [\"no\", \"n\"]:\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Please enter yes or no.\")\n",
    "        return question(question_text)\n",
    "\n",
    "def current_timestamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_model(model, filename, dir = \"models/\", add_date=True, extension = \"model\"):\n",
    "    if model is not None:\n",
    "        if add_date:\n",
    "            filename += current_timestamp()\n",
    "        filepath = f\"{dir}{filename}.{extension}\"\n",
    "        if exists(filepath):\n",
    "            if not question(f\"file {filepath} already exists. Do you want to overwrite it?\"):\n",
    "                print(\"Exiting. Model was not saved.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"Model will be overwritten.\")\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "def get_matching_models(filename, dir =\"models/\", extension = \"model\"):\n",
    "    return [f for f in listdir(\"models/\") if re.match(filename+\"\\d{8}-\\d{6}\\.\"+extension, f)]\n",
    "\n",
    "def delete_files(file_list, dir = \"\"):\n",
    "    file_list = [dir+f for f in file_list]\n",
    "    if question(f\"Following files will be deleted : {file_list}. Continue?\"):\n",
    "        for filename in file_list:\n",
    "            remove(filename)\n",
    "\n",
    "def delete_models(filename, leave=\"none\" ,dir=\"models/\", extension = \"model\"):\n",
    "    models_to_delete = get_matching_models(filename, dir, extension)\n",
    "    if leave in [\"latest\", \"last\", \"newest\", \"new\"]:\n",
    "        models_to_delete.remove(max(models_to_delete))\n",
    "    if leave in [\"old\", \"oldest\", \"first\"]:\n",
    "        models_to_delete.remove(min(models_to_delete))\n",
    "    if leave not in [\"no_date\"] and exists(f\"{dir}{filename}.{extension}\"):\n",
    "        models_to_delete.append(f\"{filename}.{extension}\")\n",
    "    delete_files(models_to_delete, dir)\n",
    "\n",
    "\n",
    "def load_model(filename, dir =\"models/\", select = \"latest\", extension = \"model\"):\n",
    "    models_with_date = get_matching_models(filename, dir, extension)\n",
    "    if select is None or select == \"none\":\n",
    "        filepath = f\"{dir}{filename}.{extension}\"\n",
    "    elif select in [\"latest\", \"last\", \"newest\", \"new\"]:\n",
    "        filepath = f\"{dir}{max(models_with_date)}\"\n",
    "    elif select in [\"first\", \"oldest\", \"old\"]:\n",
    "        filepath = f\"{dir}{min(models_with_date)}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognised select value {select}. Select can be: 'none', 'latest', 'first'\")\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        print(f\"Loading model {filepath}\")\n",
    "        return pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_processed_renttherunway_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(mu = 0, sigma = 1, x = 0):\n",
    "    return (np.exp(-0.5*\n",
    "                   ((x-mu)/sigma)**2) /\n",
    "            (sigma* np.sqrt(2*np.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSizeSimplified:\n",
    "    def __init__(self, train_data):\n",
    "        #self.train_original = train_data\n",
    "        self.train = train_data\n",
    "        self.iterations = 0 \n",
    "        #self.customers = train_data[\"user_id\"].sort_values().unique()\n",
    "        #self.articles = train_data[\"item_id\"].sort_values().unique()\n",
    "        self.number_of_customers = train_data[\"user_id\"].max()+1#self.customers.size\n",
    "        self.number_of_articles = train_data[\"item_id\"].max()+1#self.articles.size\n",
    "        self.KEPT, self.BIG, self.SMALL = 0,1,2\n",
    "        self._init_const()\n",
    "        self._init_variables()\n",
    "        self.fill_variables_in_train()\n",
    "\n",
    "    def _init_const(self):\n",
    "        self.Nc = self.train.groupby(\"user_id\")[\"user_id\"].count().values\n",
    "        self.mu_0 = df[\"size\"].mean()\n",
    "        self.sigma_0 = df[\"size\"].std()\n",
    "        self.sigma_0_inverse_square = 1/(self.sigma_0**2)\n",
    "        self.eta_kept = 0\n",
    "        self.alpha_sigma_c = (self.Nc / 2 ) + 1\n",
    "    \n",
    "    def _init_variables(self):\n",
    "        self.beta_sigma_c = np.ones_like(self.Nc)*2\n",
    "        self.mean_mu_c = np.ones_like(self.Nc)*self.mu_0\n",
    "        self.variance_mu_c = np.ones_like(self.Nc)*self.sigma_0\n",
    "        self.mean_mu_a = np.zeros(self.number_of_articles)\n",
    "        self.variance_mu_a = np.ones(self.number_of_articles)\n",
    "        self.mean_eta_small = -1\n",
    "        self.mean_eta_big = 1\n",
    "        self.variance_eta_small = 1\n",
    "        self.variance_eta_big = 1\n",
    "\n",
    "    def fill_variables_in_train(self):\n",
    "        self.fill_variables_sigma_c()\n",
    "        self.fill_variables_mu_c()\n",
    "        self.fill_variables_mu_a()\n",
    "        self.train[\"Nc\"] = self.Nc[self.train[\"user_id\"]]\n",
    "        self.fill_variables_eta_r()\n",
    "\n",
    "    def fill_variables_sigma_c(self):\n",
    "        self.train[\"alpha_to_beta\"] = self.alpha_sigma_c[self.train[\"user_id\"]] / self.beta_sigma_c[self.train[\"user_id\"]]\n",
    "    def fill_variables_mu_c(self):\n",
    "        self.train[\"mean_mu_c\"] = self.mean_mu_c[self.train[\"user_id\"]]\n",
    "        self.train[\"variance_mu_c\"]= self.variance_mu_c[self.train[\"user_id\"]]\n",
    "    def fill_variables_mu_a(self):\n",
    "        self.train[\"mean_mu_a\"] = self.mean_mu_a[self.train[\"item_id\"]]\n",
    "        self.train[\"variance_mu_a\"] = self.variance_mu_a[self.train[\"item_id\"]]\n",
    "    def fill_variables_eta_r(self):\n",
    "        self.train[\"mean_eta_r\"] = self.train[\"result\"].map({self.SMALL: self.mean_eta_small, self.BIG: self.mean_eta_big, self.KEPT: self.eta_kept})\n",
    "        self.train[\"variance_eta_r\"] = self.train[\"result\"].map({self.SMALL: self.variance_eta_small, self.BIG: self.variance_eta_big, self.KEPT: 0})\n",
    "\n",
    "    def all_converged(self):\n",
    "        return (self.converged_beta_sigma_c and\n",
    "                self.converged_mean_mu_a and self.converged_variance_mu_a and \n",
    "                self.converged_variance_mu_c and self.converged_mean_mu_c and\n",
    "                self.converged_mean_eta_r and self.converged_variance_eta_r)\n",
    "\n",
    "    def _update_and_check_variance_mu_c(self, variance_mu_c):\n",
    "        self.converged_variance_mu_c = np.allclose(self.variance_mu_c, variance_mu_c)\n",
    "        self.variance_mu_c = variance_mu_c  \n",
    "    def _update_and_check_beta_sigma_c(self, beta_sigma_c):\n",
    "        self.converged_beta_sigma_c = np.allclose(self.beta_sigma_c, beta_sigma_c)\n",
    "        self.beta_sigma_c = beta_sigma_c\n",
    "    def _update_and_check_mean_mu_c(self, mean_mu_c):\n",
    "        self.converged_mean_mu_c = np.allclose(self.mean_mu_c, mean_mu_c)\n",
    "        self.mean_mu_c = mean_mu_c\n",
    "    def _update_and_check_mean_mu_a(self, mean_mu_a):\n",
    "        self.converged_mean_mu_a = np.allclose(self.mean_mu_a, mean_mu_a)\n",
    "        self.mean_mu_a = mean_mu_a\n",
    "    def _update_and_check_variance_mu_a(self, variance_mu_a):\n",
    "        self.converged_variance_mu_a = np.allclose(self.variance_mu_a, variance_mu_a)\n",
    "        self.variance_mu_a = variance_mu_a  \n",
    "    def _update_and_check_variance_eta_r(self, small, big):\n",
    "        self.converged_variance_eta_r = np.isclose(self.variance_eta_small, small) and np.isclose(self.variance_eta_big, big)\n",
    "        self.variance_eta_small = small\n",
    "        self.variance_eta_big = big\n",
    "    def _update_and_check_mean_eta_r(self, small, big):\n",
    "        self.converged_mean_eta_r = np.isclose(self.mean_eta_small, small) and np.isclose(self.mean_eta_big, big)\n",
    "        self.mean_eta_small = small\n",
    "        self.mean_eta_big = big\n",
    "\n",
    "    def update_sigma_c(self):\n",
    "        self.train[\"expected_sigma_c\"] = ((self.train[\"size\"] \n",
    "                                           -self.train[\"mean_mu_c\"] -self.train[\"mean_mu_a\"] - self.train[\"mean_eta_r\"])**2\n",
    "                                          + (self.train[\"variance_mu_c\"]+self.train[\"variance_mu_a\"]+self.train[\"variance_eta_r\"]))\n",
    "        beta_sigma_c = 2 + 0.5*self.train.groupby('user_id')[\"expected_sigma_c\"].sum().values\n",
    "        self._update_and_check_beta_sigma_c(beta_sigma_c)\n",
    "        self.fill_variables_sigma_c()\n",
    "\n",
    "    def update_mu_c(self):\n",
    "        variance_mu_c = 1/(self.Nc*self.alpha_sigma_c/self.beta_sigma_c + self.sigma_0_inverse_square)\n",
    "        self._update_and_check_variance_mu_c(variance_mu_c)\n",
    "\n",
    "        self.train[\"expected_for_mu_c\"] = self.train[\"mean_mu_a\"]+self.train[\"mean_eta_r\"]-self.train[\"size\"]\n",
    "        sum_over_c = self.train.groupby('user_id')[\"expected_for_mu_c\"].sum().values\n",
    "        mean_mu_c = (sum_over_c*self.alpha_sigma_c/self.beta_sigma_c + self.mu_0/self.sigma_0) * self.variance_mu_c ##CHANGED added + self.mu_0/self.sigma_0\n",
    "        self._update_and_check_mean_mu_c(mean_mu_c)\n",
    "\n",
    "        self.fill_variables_mu_c()\n",
    "\n",
    "    def update_mu_a(self):\n",
    "        variance_mu_a = 1/ (1 + self.train.groupby('item_id')[\"alpha_to_beta\"].sum().values)\n",
    "        self._update_and_check_variance_mu_a(variance_mu_a)\n",
    "\n",
    "        self.train[\"expected_for_mu_a\"] = (self.train[\"mean_mu_c\"]+self.train[\"mean_eta_r\"]-self.train[\"size\"])*self.train[\"alpha_to_beta\"]\n",
    "        sum_over_a = self.train.groupby('item_id')[\"expected_for_mu_a\"].sum().values\n",
    "        mean_mu_a = sum_over_a * self.variance_mu_a\n",
    "        self._update_and_check_mean_mu_a(mean_mu_a)\n",
    "\n",
    "        self.fill_variables_mu_a()\n",
    "\n",
    "    def update_eta_r(self):\n",
    "        variance_eta_small = 1/(1+self.train[self.train[\"result\"]==self.SMALL][\"alpha_to_beta\"].sum())\n",
    "        variance_eta_big   = 1/(1+self.train[self.train[\"result\"]==self.BIG][\"alpha_to_beta\"].sum())\n",
    "        self._update_and_check_variance_eta_r(variance_eta_small, variance_eta_big)\n",
    "        self.train[\"expected_for_eta_r\"] =  (self.train[\"mean_mu_c\"]+self.train[\"mean_mu_a\"]-self.train[\"size\"])*self.train[\"alpha_to_beta\"]\n",
    "        mean_eta_small = self.variance_eta_small * (-1+ self.train[self.train[\"result\"]==self.SMALL][\"expected_for_eta_r\"].sum())\n",
    "        mean_eta_big = self.variance_eta_big * (1+ self.train[self.train[\"result\"]==self.BIG][\"expected_for_eta_r\"].sum())\n",
    "        self._update_and_check_mean_eta_r(mean_eta_small, mean_eta_big)\n",
    "        self.fill_variables_eta_r()\n",
    "        \n",
    "    def update(self):\n",
    "        self.iterations+=1\n",
    "        self.update_sigma_c()\n",
    "        self.update_mu_c()\n",
    "        self.update_mu_a()\n",
    "        self.update_eta_r()\n",
    "\n",
    "    def pdf(self, article, customer, return_status, customer_size, n_samples=1000):\n",
    "        mu_a_samples = np.random.normal(self.mean_mu_a[article], self.variance_mu_a[article], size=n_samples)\n",
    "        mu_c_samples = np.random.normal(self.mean_mu_c[customer], self.variance_mu_c[customer], size=n_samples)\n",
    "        if return_status==FIT_LABEL:\n",
    "            eta_r_samples = np.zeros(n_samples)\n",
    "        elif return_status==LARGE_LABEL:\n",
    "            eta_r_samples = np.random.normal(self.mean_eta_big, self.variance_eta_big, size=n_samples)\n",
    "        elif return_status==SMALL_LABEL:\n",
    "            eta_r_samples = np.ranodm.normal(self.mean_eta_small, self.variance_eta_small, size=n_samples)\n",
    "        else:\n",
    "            ValueError(\"unknown return status\")\n",
    "        sigma_c_samples = 1/np.random.gamma(self.alpha_sigma_c[customer], 1/self.beta_sigma_c[customer], size=n_samples)\n",
    "\n",
    "        mu_samples = mu_a_samples+mu_c_samples+eta_r_samples\n",
    "        pdf_values = normal_pdf(mu_samples, sigma_c_samples, customer_size)\n",
    "        return pdf_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_test = HierarchicalSizeSimplified(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 iterations ~ 12s\n",
    "for i in range(100):\n",
    "    hss_test.update()\n",
    "    if hss_test.all_converged():\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [25:18<00:00,  6.59it/s]\n",
      "100%|██████████| 10000/10000 [24:41<00:00,  6.75it/s]\n",
      "100%|██████████| 10000/10000 [21:57<00:00,  7.59it/s]\n",
      "100%|██████████| 10000/10000 [21:29<00:00,  7.75it/s]\n",
      "100%|██████████| 10000/10000 [21:22<00:00,  7.80it/s]\n",
      "100%|██████████| 10000/10000 [20:39<00:00,  8.07it/s]\n",
      "100%|██████████| 10000/10000 [20:40<00:00,  8.06it/s]\n",
      "100%|██████████| 10000/10000 [20:06<00:00,  8.29it/s]\n",
      "100%|██████████| 10000/10000 [20:10<00:00,  8.26it/s]\n",
      "100%|██████████| 10000/10000 [19:59<00:00,  8.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in tqdm(range(10000)):\n",
    "        hss_test.update()\n",
    "        if hss_test.all_converged():\n",
    "            print(f\"converged after {i*10000+j} iterations\")\n",
    "            break\n",
    "    save_model(hss_test, \"hss_full\", add_date=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [19:43<00:00,  8.45it/s]\n",
      "100%|██████████| 10000/10000 [19:51<00:00,  8.39it/s]\n",
      "100%|██████████| 10000/10000 [19:50<00:00,  8.40it/s]\n",
      "100%|██████████| 10000/10000 [19:49<00:00,  8.41it/s]\n",
      "100%|██████████| 10000/10000 [19:48<00:00,  8.42it/s]\n",
      "100%|██████████| 10000/10000 [19:53<00:00,  8.38it/s]\n",
      "100%|██████████| 10000/10000 [19:49<00:00,  8.41it/s]\n",
      "100%|██████████| 10000/10000 [19:58<00:00,  8.34it/s]\n",
      "100%|██████████| 10000/10000 [20:00<00:00,  8.33it/s]\n",
      "100%|██████████| 10000/10000 [19:57<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in tqdm(range(10000)):\n",
    "        hss_test.update()\n",
    "        if hss_test.all_converged():\n",
    "            print(f\"converged after {i*10000+j} iterations\")\n",
    "            break\n",
    "    save_model(hss_test, \"hss_full\", add_date=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False False False False False\n"
     ]
    }
   ],
   "source": [
    "print(hss_test.converged_beta_sigma_c,\n",
    "        hss_test.converged_mean_mu_a, hss_test.converged_variance_mu_a,\n",
    "        hss_test.converged_variance_mu_c, hss_test.converged_mean_mu_c,\n",
    "        hss_test.converged_mean_eta_r, hss_test.converged_variance_eta_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = 1\n",
    "n_samples = 10\n",
    "1/np.random.gamma(hss_test.alpha_sigma_c, \n",
    "                  1/hss_test.beta_sigma_c[customer],\n",
    "                   size=n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_test.pdf(0,0,0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.267423361921556"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss_test.mean_eta_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(hss_test, \"hss_test\", add_date=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(\"hss_test\", select=\"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7757170706561882"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss_test.mean_eta_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_test.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
