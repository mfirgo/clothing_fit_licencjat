{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_preparation import get_train_runttherunway_data, get_test_runttherunway_data, get_processed_renttherunway_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from os.path import exists\n",
    "from os import listdir, remove\n",
    "import re\n",
    "def question(question_text):\n",
    "    answer = input(question_text+ \"[y/n] \").lower()\n",
    "    if answer in [\"yes\", \"y\", \"ye\"]:\n",
    "        return True\n",
    "    if answer in [\"no\", \"n\"]:\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Please enter yes or no.\")\n",
    "        return question(question_text)\n",
    "\n",
    "def current_timestamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_model(model, filename, dir = \"models/\", add_date=True, extension = \"model\"):\n",
    "    if model is not None:\n",
    "        if add_date:\n",
    "            filename += current_timestamp()\n",
    "        filepath = f\"{dir}{filename}.{extension}\"\n",
    "        if exists(filepath):\n",
    "            if not question(f\"file {filepath} already exists. Do you want to overwrite it?\"):\n",
    "                print(\"Exiting. Model was not saved.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"Model will be overwritten.\")\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "def get_matching_models(filename, dir =\"models/\", extension = \"model\"):\n",
    "    return [f for f in listdir(\"models/\") if re.match(filename+\"\\d{8}-\\d{6}\\.\"+extension, f)]\n",
    "\n",
    "def delete_files(file_list, dir = \"\"):\n",
    "    file_list = [dir+f for f in file_list]\n",
    "    if question(f\"Following files will be deleted : {file_list}. Continue?\"):\n",
    "        for filename in file_list:\n",
    "            remove(filename)\n",
    "\n",
    "def delete_models(filename, leave=\"none\" ,dir=\"models/\", extension = \"model\"):\n",
    "    models_to_delete = get_matching_models(filename, dir, extension)\n",
    "    if leave in [\"latest\", \"last\", \"newest\", \"new\"]:\n",
    "        models_to_delete.remove(max(models_to_delete))\n",
    "    if leave in [\"old\", \"oldest\", \"first\"]:\n",
    "        models_to_delete.remove(min(models_to_delete))\n",
    "    if leave not in [\"no_date\"] and exists(f\"{dir}{filename}.{extension}\"):\n",
    "        models_to_delete.append(f\"{filename}.{extension}\")\n",
    "    delete_files(models_to_delete, dir)\n",
    "\n",
    "\n",
    "def load_model(filename, dir =\"models/\", select = \"latest\", extension = \"model\"):\n",
    "    models_with_date = get_matching_models(filename, dir, extension)\n",
    "    if select is None or select == \"none\":\n",
    "        filepath = f\"{dir}{filename}.{extension}\"\n",
    "    elif select in [\"latest\", \"last\", \"newest\", \"new\"]:\n",
    "        filepath = f\"{dir}{max(models_with_date)}\"\n",
    "    elif select in [\"first\", \"oldest\", \"old\"]:\n",
    "        filepath = f\"{dir}{min(models_with_date)}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognised select value {select}. Select can be: 'none', 'latest', 'first'\")\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        print(f\"Loading model {filepath}\")\n",
    "        return pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(hss_test, \"hss_test\", add_date=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_models(\"hss_test\", leave=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_processed_renttherunway_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSizeSimplified:\n",
    "    def __init__(self, train_data):\n",
    "        #self.train_original = train_data\n",
    "        self.train = train_data\n",
    "        self.iterations = 0 \n",
    "        #self.customers = train_data[\"user_id\"].sort_values().unique()\n",
    "        #self.articles = train_data[\"item_id\"].sort_values().unique()\n",
    "        self.number_of_customers = train_data[\"user_id\"].max()+1#self.customers.size\n",
    "        self.number_of_articles = train_data[\"item_id\"].max()+1#self.articles.size\n",
    "        self.KEPT, self.BIG, self.SMALL = 0,1,2\n",
    "        self._init_const()\n",
    "        self._init_variables()\n",
    "        self.fill_variables_in_train()\n",
    "\n",
    "    def _init_const(self):\n",
    "        self.Nc = self.train.groupby(\"user_id\")[\"user_id\"].count().values\n",
    "        self.mu_0 = df[\"size\"].mean()\n",
    "        self.sigma_0 = df[\"size\"].std()\n",
    "        self.sigma_0_inverse_square = 1/(self.sigma_0**2)\n",
    "        self.eta_kept = 0\n",
    "        self.alpha_sigma_c = (self.Nc / 2 ) + 1\n",
    "    \n",
    "    def _init_variables(self):\n",
    "        self.beta_sigma_c = np.ones_like(self.Nc)*2\n",
    "        self.mean_mu_c = np.ones_like(self.Nc)*self.mu_0\n",
    "        self.variance_mu_c = np.ones_like(self.Nc)*self.sigma_0\n",
    "        self.mean_mu_a = np.zeros(self.number_of_articles)\n",
    "        self.variance_mu_a = np.ones(self.number_of_articles)\n",
    "        self.mean_eta_small = -1\n",
    "        self.mean_eta_big = 1\n",
    "        self.variance_eta_small = 1\n",
    "        self.variance_eta_big = 1\n",
    "\n",
    "    def fill_variables_in_train(self):\n",
    "        self.fill_variables_sigma_c()\n",
    "        self.fill_variables_mu_c()\n",
    "        self.fill_variables_mu_a()\n",
    "        self.train[\"Nc\"] = self.Nc[self.train[\"user_id\"]]\n",
    "        self.fill_variables_eta_r()\n",
    "\n",
    "    def fill_variables_sigma_c(self):\n",
    "        self.train[\"alpha_to_beta\"] = self.alpha_sigma_c[self.train[\"user_id\"]] / self.beta_sigma_c[self.train[\"user_id\"]]\n",
    "    def fill_variables_mu_c(self):\n",
    "        self.train[\"mean_mu_c\"] = self.mean_mu_c[self.train[\"user_id\"]]\n",
    "        self.train[\"variance_mu_c\"]= self.variance_mu_c[self.train[\"user_id\"]]\n",
    "    def fill_variables_mu_a(self):\n",
    "        self.train[\"mean_mu_a\"] = self.mean_mu_a[self.train[\"item_id\"]]\n",
    "        self.train[\"variance_mu_a\"] = self.variance_mu_a[self.train[\"item_id\"]]\n",
    "    def fill_variables_eta_r(self):\n",
    "        self.train[\"mean_eta_r\"] = self.train[\"result\"].map({self.SMALL: self.mean_eta_small, self.BIG: self.mean_eta_big, self.KEPT: self.eta_kept})\n",
    "        self.train[\"variance_eta_r\"] = self.train[\"result\"].map({self.SMALL: self.variance_eta_small, self.BIG: self.variance_eta_big, self.KEPT: 0})\n",
    "\n",
    "    def all_converged(self):\n",
    "        return (self.converged_beta_sigma_c and\n",
    "                self.converged_mean_mu_a and self.converged_variance_mu_a and \n",
    "                self.converged_variance_mu_c and self.converged_mean_mu_c and\n",
    "                self.converged_mean_eta_r and self.converged_variance_eta_r)\n",
    "\n",
    "    def _update_and_check_variance_mu_c(self, variance_mu_c):\n",
    "        self.converged_variance_mu_c = np.allclose(self.variance_mu_c, variance_mu_c)\n",
    "        self.variance_mu_c = variance_mu_c  \n",
    "    def _update_and_check_beta_sigma_c(self, beta_sigma_c):\n",
    "        self.converged_beta_sigma_c = np.allclose(self.beta_sigma_c, beta_sigma_c)\n",
    "        self.beta_sigma_c = beta_sigma_c\n",
    "    def _update_and_check_mean_mu_c(self, mean_mu_c):\n",
    "        self.converged_mean_mu_c = np.allclose(self.mean_mu_c, mean_mu_c)\n",
    "        self.mean_mu_c = mean_mu_c\n",
    "    def _update_and_check_mean_mu_a(self, mean_mu_a):\n",
    "        self.converged_mean_mu_a = np.allclose(self.mean_mu_a, mean_mu_a)\n",
    "        self.mean_mu_a = mean_mu_a\n",
    "    def _update_and_check_variance_mu_a(self, variance_mu_a):\n",
    "        self.converged_variance_mu_a = np.allclose(self.variance_mu_a, variance_mu_a)\n",
    "        self.variance_mu_a = variance_mu_a  \n",
    "    def _update_and_check_variance_eta_r(self, small, big):\n",
    "        self.converged_variance_eta_r = np.isclose(self.variance_eta_small, small) and np.isclose(self.variance_eta_big, big)\n",
    "        self.variance_eta_small = small\n",
    "        self.variance_eta_big = big\n",
    "    def _update_and_check_mean_eta_r(self, small, big):\n",
    "        self.converged_mean_eta_r = np.isclose(self.mean_eta_small, small) and np.isclose(self.mean_eta_big, big)\n",
    "        self.mean_eta_small = small\n",
    "        self.mean_eta_big = big\n",
    "\n",
    "    def update_sigma_c(self):\n",
    "        self.train[\"expected_sigma_c\"] = ((self.train[\"size\"] \n",
    "                                           -self.train[\"mean_mu_c\"] -self.train[\"mean_mu_a\"] - self.train[\"mean_eta_r\"])**2\n",
    "                                          + (self.train[\"variance_mu_c\"]+self.train[\"variance_mu_a\"]+self.train[\"variance_eta_r\"]))\n",
    "        beta_sigma_c = 2 + 0.5*self.train.groupby('user_id')[\"expected_sigma_c\"].sum().values\n",
    "        self._update_and_check_beta_sigma_c(beta_sigma_c)\n",
    "        self.fill_variables_sigma_c()\n",
    "\n",
    "    def update_mu_c(self):\n",
    "        variance_mu_c = 1/(self.Nc*self.alpha_sigma_c/self.beta_sigma_c + self.sigma_0_inverse_square)\n",
    "        self._update_and_check_variance_mu_c(variance_mu_c)\n",
    "\n",
    "        self.train[\"expected_for_mu_c\"] = self.train[\"mean_mu_a\"]+self.train[\"mean_eta_r\"]-self.train[\"size\"]\n",
    "        sum_over_c = self.train.groupby('user_id')[\"expected_for_mu_c\"].sum().values\n",
    "        mean_mu_c = (sum_over_c*self.alpha_sigma_c/self.beta_sigma_c + self.mu_0/self.sigma_0) * self.variance_mu_c ##CHANGED added + self.mu_0/self.sigma_0\n",
    "        self._update_and_check_mean_mu_c(mean_mu_c)\n",
    "\n",
    "        self.fill_variables_mu_c()\n",
    "\n",
    "    def update_mu_a(self):\n",
    "        variance_mu_a = 1/ (1 + self.train.groupby('item_id')[\"alpha_to_beta\"].sum().values)\n",
    "        self._update_and_check_variance_mu_a(variance_mu_a)\n",
    "\n",
    "        self.train[\"expected_for_mu_a\"] = (self.train[\"mean_mu_c\"]+self.train[\"mean_eta_r\"]-self.train[\"size\"])*self.train[\"alpha_to_beta\"]\n",
    "        sum_over_a = self.train.groupby('item_id')[\"expected_for_mu_a\"].sum().values\n",
    "        mean_mu_a = sum_over_a * self.variance_mu_a\n",
    "        self._update_and_check_mean_mu_a(mean_mu_a)\n",
    "\n",
    "        self.fill_variables_mu_a()\n",
    "\n",
    "    def update_eta_r(self):\n",
    "        variance_eta_small = 1/(1+self.train[self.train[\"result\"]==self.SMALL][\"alpha_to_beta\"].sum())\n",
    "        variance_eta_big   = 1/(1+self.train[self.train[\"result\"]==self.BIG][\"alpha_to_beta\"].sum())\n",
    "        self._update_and_check_variance_eta_r(variance_eta_small, variance_eta_big)\n",
    "        self.train[\"expected_for_eta_r\"] =  (self.train[\"mean_mu_c\"]+self.train[\"mean_mu_a\"]-self.train[\"size\"])*self.train[\"alpha_to_beta\"]\n",
    "        mean_eta_small = self.variance_eta_small * (-1+ self.train[self.train[\"result\"]==self.SMALL][\"expected_for_eta_r\"].sum())\n",
    "        mean_eta_big = self.variance_eta_big * (1+ self.train[self.train[\"result\"]==self.BIG][\"expected_for_eta_r\"].sum())\n",
    "        self._update_and_check_mean_eta_r(mean_eta_small, mean_eta_big)\n",
    "        self.fill_variables_eta_r()\n",
    "        \n",
    "    def update(self):\n",
    "        self.iterations+=1\n",
    "        self.update_sigma_c()\n",
    "        self.update_mu_c()\n",
    "        self.update_mu_a()\n",
    "        self.update_eta_r()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hss_test = HierarchicalSizeSimplified(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 iterations ~ 12s\n",
    "for i in range(100):\n",
    "    hss_test.update()\n",
    "    if hss_test.all_converged():\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.119018449959658"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss_test.mean_eta_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(hss_test, \"hss_test\", add_date=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models/hss_test20220715-103733.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HierarchicalSizeSimplified at 0x7f5bd8234fa0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"hss_test\", select=\"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.346381647110297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss_test.mean_eta_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_original</th>\n",
       "      <th>user_id_original</th>\n",
       "      <th>item_id_original</th>\n",
       "      <th>size</th>\n",
       "      <th>review_date</th>\n",
       "      <th>category</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>result</th>\n",
       "      <th>alpha_to_beta</th>\n",
       "      <th>...</th>\n",
       "      <th>variance_mu_c</th>\n",
       "      <th>mean_mu_a</th>\n",
       "      <th>variance_mu_a</th>\n",
       "      <th>Nc</th>\n",
       "      <th>mean_eta_r</th>\n",
       "      <th>variance_eta_r</th>\n",
       "      <th>expected_sigma_c</th>\n",
       "      <th>expected_for_mu_c</th>\n",
       "      <th>expected_for_mu_a</th>\n",
       "      <th>expected_for_eta_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>2260466</td>\n",
       "      <td>14</td>\n",
       "      <td>April 20, 2016</td>\n",
       "      <td>romper</td>\n",
       "      <td>44334</td>\n",
       "      <td>4396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>...</td>\n",
       "      <td>66.740319</td>\n",
       "      <td>0.148828</td>\n",
       "      <td>0.956167</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6617.741781</td>\n",
       "      <td>-14.325771</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.015293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>153475</td>\n",
       "      <td>12</td>\n",
       "      <td>June 18, 2013</td>\n",
       "      <td>gown</td>\n",
       "      <td>28835</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>...</td>\n",
       "      <td>70.576094</td>\n",
       "      <td>10.354689</td>\n",
       "      <td>0.848392</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9624.550840</td>\n",
       "      <td>-4.008021</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.031157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>1063761</td>\n",
       "      <td>4</td>\n",
       "      <td>December 14, 2015</td>\n",
       "      <td>sheath</td>\n",
       "      <td>37976</td>\n",
       "      <td>1945</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>...</td>\n",
       "      <td>70.539116</td>\n",
       "      <td>1.140773</td>\n",
       "      <td>0.890603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9400.358697</td>\n",
       "      <td>-5.125536</td>\n",
       "      <td>0.031123</td>\n",
       "      <td>0.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>126335</td>\n",
       "      <td>8</td>\n",
       "      <td>February 12, 2014</td>\n",
       "      <td>dress</td>\n",
       "      <td>96080</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>71.105221</td>\n",
       "      <td>24.516869</td>\n",
       "      <td>0.617642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14549.615920</td>\n",
       "      <td>17.632552</td>\n",
       "      <td>0.019532</td>\n",
       "      <td>0.024586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>616682</td>\n",
       "      <td>12</td>\n",
       "      <td>September 26, 2016</td>\n",
       "      <td>gown</td>\n",
       "      <td>15959</td>\n",
       "      <td>1032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>...</td>\n",
       "      <td>65.389103</td>\n",
       "      <td>2.574426</td>\n",
       "      <td>0.961189</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7024.076948</td>\n",
       "      <td>-9.419857</td>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.017124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192539</th>\n",
       "      <td>fit</td>\n",
       "      <td>66386</td>\n",
       "      <td>2252812</td>\n",
       "      <td>8</td>\n",
       "      <td>May 18, 2016</td>\n",
       "      <td>jumpsuit</td>\n",
       "      <td>7026</td>\n",
       "      <td>4382</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>...</td>\n",
       "      <td>66.366476</td>\n",
       "      <td>-0.161312</td>\n",
       "      <td>0.887071</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7614.283171</td>\n",
       "      <td>-7.975646</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.015004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192540</th>\n",
       "      <td>fit</td>\n",
       "      <td>118398</td>\n",
       "      <td>682043</td>\n",
       "      <td>4</td>\n",
       "      <td>September 30, 2016</td>\n",
       "      <td>dress</td>\n",
       "      <td>12494</td>\n",
       "      <td>1164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>...</td>\n",
       "      <td>69.793032</td>\n",
       "      <td>-1.624764</td>\n",
       "      <td>0.702435</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9214.119204</td>\n",
       "      <td>-4.925670</td>\n",
       "      <td>0.015162</td>\n",
       "      <td>0.014908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192541</th>\n",
       "      <td>fit</td>\n",
       "      <td>47002</td>\n",
       "      <td>683251</td>\n",
       "      <td>8</td>\n",
       "      <td>March 4, 2016</td>\n",
       "      <td>dress</td>\n",
       "      <td>5019</td>\n",
       "      <td>1166</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>...</td>\n",
       "      <td>4.245548</td>\n",
       "      <td>-3.663509</td>\n",
       "      <td>0.648229</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.206876</td>\n",
       "      <td>-8.231496</td>\n",
       "      <td>-0.082414</td>\n",
       "      <td>-0.116253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192542</th>\n",
       "      <td>fit</td>\n",
       "      <td>961120</td>\n",
       "      <td>126335</td>\n",
       "      <td>16</td>\n",
       "      <td>November 25, 2015</td>\n",
       "      <td>dress</td>\n",
       "      <td>101534</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>...</td>\n",
       "      <td>70.945454</td>\n",
       "      <td>24.516869</td>\n",
       "      <td>0.617642</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12611.361685</td>\n",
       "      <td>9.632552</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.026383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192543</th>\n",
       "      <td>fit</td>\n",
       "      <td>123612</td>\n",
       "      <td>127865</td>\n",
       "      <td>16</td>\n",
       "      <td>August 29, 2017</td>\n",
       "      <td>gown</td>\n",
       "      <td>13040</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>...</td>\n",
       "      <td>70.780145</td>\n",
       "      <td>20.120043</td>\n",
       "      <td>0.730779</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11077.341592</td>\n",
       "      <td>2.746017</td>\n",
       "      <td>0.023304</td>\n",
       "      <td>0.028751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192544 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       result_original  user_id_original  item_id_original  size  \\\n",
       "0                  fit            420272           2260466    14   \n",
       "1                  fit            273551            153475    12   \n",
       "2                  fit            360448           1063761     4   \n",
       "3                  fit            909926            126335     8   \n",
       "4                  fit            151944            616682    12   \n",
       "...                ...               ...               ...   ...   \n",
       "192539             fit             66386           2252812     8   \n",
       "192540             fit            118398            682043     4   \n",
       "192541             fit             47002            683251     8   \n",
       "192542             fit            961120            126335    16   \n",
       "192543             fit            123612            127865    16   \n",
       "\n",
       "               review_date  category  user_id  item_id  result  alpha_to_beta  \\\n",
       "0           April 20, 2016    romper    44334     4396       0       0.000188   \n",
       "1            June 18, 2013      gown    28835       65       0       0.000312   \n",
       "2        December 14, 2015    sheath    37976     1945       0       0.000319   \n",
       "3        February 12, 2014     dress    96080        7       0       0.000206   \n",
       "4       September 26, 2016      gown    15959     1032       0       0.000205   \n",
       "...                    ...       ...      ...      ...     ...            ...   \n",
       "192539        May 18, 2016  jumpsuit     7026     4382       0       0.000173   \n",
       "192540  September 30, 2016     dress    12494     1164       0       0.000157   \n",
       "192541       March 4, 2016     dress     5019     1166       0       0.009237   \n",
       "192542   November 25, 2015     dress   101534        7       0       0.000238   \n",
       "192543     August 29, 2017      gown    13040       10       0       0.000271   \n",
       "\n",
       "        ...  variance_mu_c  mean_mu_a  variance_mu_a  Nc  mean_eta_r  \\\n",
       "0       ...      66.740319   0.148828       0.956167   6         0.0   \n",
       "1       ...      70.576094  10.354689       0.848392   1         0.0   \n",
       "2       ...      70.539116   1.140773       0.890603   1         0.0   \n",
       "3       ...      71.105221  24.516869       0.617642   1         0.0   \n",
       "4       ...      65.389103   2.574426       0.961189   7         0.0   \n",
       "...     ...            ...        ...            ...  ..         ...   \n",
       "192539  ...      66.366476  -0.161312       0.887071   7         0.0   \n",
       "192540  ...      69.793032  -1.624764       0.702435   3         0.0   \n",
       "192541  ...       4.245548  -3.663509       0.648229  24         0.0   \n",
       "192542  ...      70.945454  24.516869       0.617642   1         0.0   \n",
       "192543  ...      70.780145  20.120043       0.730779   1         0.0   \n",
       "\n",
       "        variance_eta_r  expected_sigma_c  expected_for_mu_c  \\\n",
       "0                  0.0       6617.741781         -14.325771   \n",
       "1                  0.0       9624.550840          -4.008021   \n",
       "2                  0.0       9400.358697          -5.125536   \n",
       "3                  0.0      14549.615920          17.632552   \n",
       "4                  0.0       7024.076948          -9.419857   \n",
       "...                ...               ...                ...   \n",
       "192539             0.0       7614.283171          -7.975646   \n",
       "192540             0.0       9214.119204          -4.925670   \n",
       "192541             0.0        117.206876          -8.231496   \n",
       "192542             0.0      12611.361685           9.632552   \n",
       "192543             0.0      11077.341592           2.746017   \n",
       "\n",
       "        expected_for_mu_a  expected_for_eta_r  \n",
       "0                0.015265            0.015293  \n",
       "1                0.027931            0.031157  \n",
       "2                0.031123            0.031487  \n",
       "3                0.019532            0.024586  \n",
       "4                0.016596            0.017124  \n",
       "...                   ...                 ...  \n",
       "192539           0.015032            0.015004  \n",
       "192540           0.015162            0.014908  \n",
       "192541          -0.082414           -0.116253  \n",
       "192542           0.020553            0.026383  \n",
       "192543           0.023304            0.028751  \n",
       "\n",
       "[192544 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss_test.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
